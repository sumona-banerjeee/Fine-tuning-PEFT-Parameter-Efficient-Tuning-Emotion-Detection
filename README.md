# Fine-tuning & PEFT (Parameter-Efficient Tuning)

**Objective:**  
Compare full fine-tuning versus LoRA/PEFT methods on a domain-specific dataset Emotion Detection.

---

## Deliverable:  
- Benchmark table comparing:
  - Model performance (accuracy, F1, etc.)  
  - Training cost (time, GPU hours)  
  - Inference latency (speed)

---

## Tech:  
Hugging Face PEFT, LoRA, PyTorch

---

## Challenge Level:  
Explores trade-offs between compute resources and model quality.
